  \documentclass[12pt,oneside]{article}
  \title{Discussion of rough volatility modeling and its computational accomplishment.}
  \date{ March 2021 }
  \author{Rachel Dance, Tim Howes, Silvia Cecilia Hernández Vargas, Finlay Young}
  \usepackage[rsfm,fancyhdr,hyperref,colour]{edmaths_mod}
  \flushbottom

  \begin{document}
  \pagenumbering{roman}
  \maketitle

  \begin{abstract}In this work we focus on the historical and theoretical background to the current research area of rough volatility. Attention is paid to the advantages of such a scheme and we emphasise the implementation of such schemes, which have recently made them accessible to industrial settings. Inspired by Oumgari et al.,(2019) \cite{jacquier2020deep} 
  \emph{Abstract - a very concise text which roughly summarizes contents of the document.  Almost never longer than half a page, usually much shorter (200-300 words).}
   \end{abstract}


  \tableofcontents
 % \addcontentsline{toc}{Contents}
 \newpage
 \pagenumbering{arabic}

\section{Introduction}
\emph{The aim of this document is to provide some guidance as to how a good report or paper might look like. 
This is by no means an  exhaustive discussion, but some basics are covered. 
In the introduction we describe the importance of the problem we address in the paper. We should also place our piece in the context of related work already present in the literature (including recent achievements), citing the sources carefully and precisely.
We should provide a (brief) overview of what is done in the paper. 
An informal statement of the problem is usually welcome.}

Estimating volatility has become of a great importance in the financial market. In fact, traders tend to talk about volatility of the assets instead of their derivatives prices [do we have proof of this? is it anecdotal?]. Empirical experience has made evident that volatility is not a parameter that can remain constant [reference evidence!!]. In fact, if it were constant, the purpose of derivative's contract would be undermined. Thus, having a model which can correctly try to explain its process is desirable. Some of the characteristics seen in the volatility is that it tends to increase or decrease for a certain time period to finally return to a certain mean level. The impact of favourable news such as non- favourable news have a different magnitude in its value.


\emph{We consider a new generation of stochastic volatility models, dubbed by Jim Gatheral, Thibault Jaisson and Mathieu Rosenbaum as `rough volatility models’, where the instantaneous volatility is driven by a (rough) fractional Brownian motion} %https://sites.google.com/site/roughvol/ 

\emph{Our main result is
that log-volatility behaves essentially as a fractional Brownian motion
with Hurst exponent H of order 0.1, at any reasonable time scale.
This leads us to adopt the fractional stochastic volatility (FSV) model
of Comte and Renault [16]. We call our model Rough FSV (RFSV)
to underline that, in contrast to FSV, H < 1/2.}

Long memory volatility referred to the slow decay of the autocorrelation function.

Things to remember when writing introductions:
\begin{enumerate} %in other words, a list!!
\item Keep it short.
\item Provide context of your work. Cite literature.
\item Be precise. Avoid `big' words, such `novel', `revolutionary', `ultimate': never suggest that the paper contains more than it does.
\item Do provide an overview of what is done in the paper.
\item It is the last part to write.
\end{enumerate}

REQUIRED: The rest of the paper is structured as follows. In Section~\ref{Division into sections} we discuss how to split your documents into sections, in Section~\ref{Referencing}, we examine  managing  references. In Section~\ref{formatting}, we share some tips on formatting. We conclude with Section~\ref{conclusion} in which we provide some final remarks.'



\section{Black Scholes model and theoretical foundations}
\label{sec:black_scholes_foundations}
The Black and Scholes (B&S) model was first introduced several decades ago, and is arguably the most well known option pricing model, and it is a fundamental building block of almost all pricing models \cite{BlackScholesOR}. Despite the fact that it underpins (on some level) almost all asset pricing models developed since, it is rarely used directly in its pure form due to its well known shortcomings. We give a short overview here. 

The price of an asset ($S_t$) at the current time is not generally expected to be the same as it is currently. Broadly speaking, an asset's value will grow (or shrink) on average by some rate $\mu \in [0,1]$ (under the the probability measure \mathbb P). The formula also contains a volatility term $\sigma$, representing the variability of the price of an asset in the short term. The model takes the form of Equation \ref{eqn:black_and_scholes}.

\begin{equation}
\label{eqn:black_and_scholes} 
dS_t=\mu dt + \sigma dW
\end{equation}
Where $\mu$ is a rate of growth, and $\sigma$ is the pivotal volatility term which represents the short term variability of an asset price. In this model, the asset price $S_t$ is a stochastic process which is continuous in time. Equation \ref{eqn:black_and_scholes} shows the volatility is multiplied by a dW term, where W is a Wiener process (Brownian Motion). As such, the asset price $S_t$ is the logarithm and follows a Brownian Motion, and the dt term represents the drift.

The key shortcomings of the model are firstly that the $\mu$ and $\sigma$ are represented either as constant, or as a deterministic function of time \cite{BlackScholesOR, Gatheral2014}. This is one of the main shortfalls of the model. \emph{Further, the model is designed with European type options in mind, where there is no option for an early exercise date.} 

As an improvement to this constant volatility, the volatility smile was introduced in which the volatility is no longer a constant. This further led to the development of new models to take into account these new dynamics in the market, and to make adjustments to pricing strategies.

\subsection{Types of volatility models}

The volatility of an asset can be defined as the fluctuation or dispersion with respect of its mean tendency per unit of time. Models for estimating volatility can be divided in two types depending on its way of calculation as: i) Historic estimation and ii) Implied volatility estimation.

\textbf{Implied volatility models:}
\\

Also known as forward-looking models. Their principal output are the option prices observed in the market. Therefore, it can be thought as the volatility value needed to make a model pricing, such as Black-Scholes-Merton, to have the same option price as the price observed in the market. Plotting implied volatility as a function of strike price and time to expiry generates the volatility surface. If we would like to observe the dynamics of the volatility, we could use the Dupire’s local volatility model, where the local volatility $\sigma(Y_{t}, t)$ is a deterministic function of the underlying price and time.  

\textbf{Historic estimation:} 
\\

Also known as models of backward-looking, where the principal input for its estimation is the historical data of the asset prices. Among this type of estimation we can also divide the models as: i) Punctual measures and ii) Series measures. The latter can also be divided in parametric models and non parametric models.
 \\
 
Punctual measure is the simplest estimation as it can only be the standard deviation of the returns of a given asset. However, the main disadvantage is that the evolution of this parameter is not taken into consideration, as it will remain constant for different periods of time. 
\\

Therefore it is immediate to assume that considering the evolution of a given time period will generate a better accuracy of this parameter. For example, the models of autoregressive conditional heteroscedasticity such as ARCH and GARCH models assume the volatility to be a function of the return of the prices but also of the volatility itself for previous periods of time. These type of models are widely used and have been developed for example to ensure an asymmetric volatility on positive and negative returns (e.g EGARCH model).
%https://file.scirp.org/pdf/JMF_2017051916361813.pdf
\\

Non parametric models have the great advantage of not assumming a given distribution of the data. However, a disadvantage is that a great amount of data to fit correctly the model, which in real life it can be hard to access for. Examples for this estimation can be neuronal networks, kernel regressions, etc.
\\

Finally, parametric models assume a distribution of our data. For example, Brownian motion assume a normal distribution for the return of the prices. Stochastic models are part of this type, and even though it was not quite often used because of its complexity, now we can use computational power to .....
\\

Stochastic models:

Black-Scholes framework, the volatility function is either constant or a deterministic function of time.  Notable amongst such stochastic volatility models are the Hull and White model [32], the
Heston model [31], and the SABR model [29]. Whilst stochastic volatility
dynamics are more realistic than local volatility dynamics, generated option
prices are not consistent with observed European option prices

Fact that "smile dynamics" is poorly predicted by local vol models leading to bad Hedging of exotic options.

More recent market practice is to use local-stochastic-volatility
(LSV) models which both fit the market exactly and generate reasonable
dynamics.




\subsection{The Heston Model}
The Heston model is a stochastic volatility model which allows for European option pricing, developed as an improvement on the Black-Scholes (B-S) Model. The main difference the Heston model has to the B-S model is the additional randomness introduce to model the volatility of the underlying stock. In the B-S model, volatility is stated to be a constant value, which provides for a simple model but it not representative of a realistic model of an assets volatility in the market, as we know that volatility is not constant. In the Heston model the underlying assets volatility is modelled by the Cox-Ingersoll-Ross model (commonly used as an interest rate model). This additional complexity brings with it additional parameters required for the model, most notably there are two Brownian motions, one corresponding to the asset price ($W^s$), and one corresponding to the asset variance($W^v$), and the requirement to accurately calibrate these additional parameters to provide an accurate model.The Heston model is shown below: 

$$dS_t= S_t(\mu dt + \sqrt{v_t} dW_t^{s})$$ where the variance is expressed as: $$dv_t = \kappa (\theta - v_t)dt + \xi\sqrt{v_t}dW_t^{v}$$
As we can see in the above, the parameters within the Heston model which replace $\sigma$ within the B-S model are: $\xi$, the Volatility of volatility which allows for control of the volatility smiles curvature, $\kappa$ is the rate at which $v_t$ returns to 0 (the mean reversion), and $\theta$ which is the long-running price variance. 
\subsection{PPDE and CPDE in general}
Rach?
\subsection{Feynman-Kac}
No idea yet?!

\section{Pricing}
\label{sec:pricing}


\section{Fractional Brownian Motion}
\label{sec:fractionalBm}
In rough volatility models, the volatility of a given asset at any time point is the solution to an SDE driven by a Fractional Brownian Motion (fBM). The inclusion of fBM comes from the analysis of empirical time series data which suggests that the volatility process is non-Markovian (\cite{redGATHERAL}). When we say a process is 'non-Markovian', we roughly mean that the process in a given state depends on more than one previous state of the process. On the other hand, a process that is Markovian in a given state depends only on the previous state of the process. By looking at the properties of fBM, we can see how it would be preferred over classical Brownian motion in such models and how it satisifes the non-Markovian time series property. fBM can be written as a stochastic process $(\textit{$W^H_t$})_{t\ge0}$ where \textit{H} is called the Hurst parameter with $\textit{H} \in (0,1)$. Similar to the classical Brownian motion, it has the following properties: 
\begin{itemize} \item The process is Gaussian and continuous in time. \item $(\textit{$W^H_0$})=0$  \item $\mathbb{E}$[\textit{$W^H_t$}]$=0$ \ \  $\forall t \ge 0$ \end{itemize}
It differs from the classical Brownian motion as it has the following covariance function for $0 < t_1 < t_2$: $$\mathbb{E}[\textit{$W^H_{t_1}$} \textit{$W^H_{t_2}$}] = 1/2 (|t_1|^{2H}+|t_2|^{2H}-|t_1-t_2|^{2H})$$ If $H=1/2$ the covariance equals $0$, which means the increments are not correlated, giving us the classical Brownian motion. If $H<1/2$ then the covariance is negative, meaning the increments of the process are negatively correlated. If $H>1/2$ then the covariance is positive, meaning the increments of the process are positively correlated.  Therefore, by setting $H\in(0,1)\setminus\{\frac{1}{2}\}$, we allow for dependence between the increments of fBM, making the volatility process non-Markovian.  In rough volatility models, \textit{H} is generally chosen to be less than 1/2 as this is shown to be consistent with time series data (\cite{GATHERAL}) which we will discuss next.
\\
\\
Comte and Renault [16] proposed to model log-volatility using fBM, in order to ensure a long memory property by choosing the Hurst parameter to be H > 1/2, i.e increments of the fBM are positively correlated, this is called the Fractional Stochastic Volatility (FSV). However, using the log- volatility with H < 1/2 of the fBM, this model is now potential to be not only consistent with properties observed for time series but also consistent with the shape of the volatility surface. This modification is called the Rough Fractional Stochastic Volatility (RFSV).


\subsection{Evidence of rough volatility}
\label{sec:rough_vol_evidence}
The main motivation for developing what we now know as "rough volatility" was to produce a model which would bridge the gap between the volatility surface generated by conventional stochastic volatility models to the surfaces generated by observed (historical) volatility. 
\\
\\
Evidence to support the accuracy of the rough volatility  has been well explored in \cite{BlackScholesOR, Gatheral2014} in which the smoothness of the log-volatility process as well as it's increments for selected assets were investigated. It was shown that, empirically, the increments of the log-volatility of various assets exhibited a scaling property in its expectation with a constant smoothness parameter that is given by $$\mathbb{E}[|log(\sigma_\Delta)-log(\sigma_0)|^q]=K_q\nu^q\Delta^{qH}$$ where $\Delta$ is the increment size, $q>0$ and $\nu>0$. The distribution of the increments was also shown to be approximately Gaussian which led to a proposed model for the rough volatility process using fractional Brownian motion given by: $$\sigma_t = exp(X_t),$$ where $$X_t=\vega\int_{-\infty}^t e^{-(t-s)\alpha}dW_t^H+m, \ \ \ m\in\mathbb{R}, \ \  \nu, \alpha>0$$ and $0<H<1/2$ is the measured smoothness of the volatility. This model was coined as the RFSV (Rough Fractional Stochastic Volatility) model. 
\\
\\
By comparing the smoothness of simulated data from this model to that of empirical data, it provided significant evidence that volatility is indeed rough. A key part of this analysis was comparing the actual volatility of the S\&P over a 3500 day period with the volatility process generated by the model over the same time-frame. The graphs of this analysis are shown below.
\\
\\
As you can see from the above plots, the dynamics of the two processes are very similar. They both exhibit the trend that that periods of high volatility are followed by periods of low volatility and vice versa. Recall that choosing $H<1/2$ means that the increments of the fractional Brownian motion are negatively correlated. In this analysis, $H$ was chosen to be less than 1/2 which would capture this certain trend in the model. Another key part of this analysis was that they were able to capture the smoothness of the volatility process with this model by tuning $H$. 

\subsection{The Rough Heston volatility Model}
\label{sebsec:rough_heston}
The classical Heston model has been introduced, as have the pitfalls  of these simplistic early option pricing models. The Classical Heston model does not agree with time series in practise, and does not generate a volatility surface similar to that observed, however there are benefits to the various parameters which make the model tractable, and when applying a rough volatility framework. Small Hurst parameters & fBM are introduced and the Ricatti characteristic function can be replaced with with a fractional Ricatti characteristic function to form the rough Heston model to produce behaviour seen in both historical & implied volatility.

A kernel, $(t-u)^{\alpha-1}$, is introduced to the equation to represent the roughness on the Heston model. 

$$v_t = v_0 + \frac{1}{\Gamma(\alpha)} \int_{0}^{t} (t-u)^{\alpha-1} \kappa (\theta - v_u)du + \frac{1}{\Gamma(\alpha)} \int_{0}^{t} \xi\sqrt{v_t}dW_u^{v}$$

It is important to note that when $\alpha$ = 1 when achieve the classical Heston Model. ...

The classical model allows for numerical calibration, however in the rough Heston model requires intricate calibration for the parameters given that fractional volatility models have a reliance Monte-Carlo simulations 

\section{Computational Advancements in Pricing with Rough Volatility Models}
\label{sec:comp_advamcement}

As discussed in (\cite{OUMGARI}),  the use of fBM in such models carries a computational burden with it. In terms of option pricing, this restricts the use of several pricing tools such as the Black-Scholes formula and other pricing PDE's due to the non-deterministic nature of the volatility process.  Pricing via Monte Carlo simulation of the asset price and volatility processes is perhaps the one pricing tool that works but it can be slow in approximating continuous time solutions within a sufficient level of accuracy, especially for simulating non-Markovian process due to the extra memory required to store previous process states. Here, we give a brief overview of some theories that were introduced to combat the computational expense of exact Monte Carlo simulation of the volatility process. Namely, we discuss the Hybrid scheme from (\cite{BENNEDSON}), the Markovian representation of fBM in (\cite{HARMS}) and the extension of the Donsker theorem to fBM in (\cite{MUGURUZA}). These methods were introduced to improve the efficiency of the simulation of fBM and volatility processes while maintaining a reasonable level of accuracy.

\subsection{The Hybrid Scheme}
\label{subsec:hybrid_scheme}
The Hybrid Scheme was introduced by (\cite{BENNEDSON}) in (\cite{PAPER}) in which they study simulation methods for Brownian semistationary processes. A brownian semistationary process can be written as $$X(t)=\int_{-\infty}^t g(t-s) \sigma(s) dW(s),  \ \  t\in\mathbb{R}$$ where $\sigma$ is a predictable process with respect to the given filatration. It is mentioned in the paper that when $g(x) \propto x^\alpha$ and $\alpha \in (-\frac{1}{2}, \frac{1}{2}) \setminus \{0\}$, the process behaves (locally) like a fBM with Hurst paramter $H=\alpha+1/2$.  Therefore, being able to efficiently simulate such a process would help to improve computational efficiency for pricing in some rough volatility models.  
\\
\\
In the referenced paper, a method for simulating such a process was introduced. The given scheme involves approximating the function $g$ using a step function except for near 0, where a power function is used for approximation. Through a series of derivations,  the resulting discretisation scheme is written as a linear combination of a Riemann sum and Wiener integrals. The scheme is given by: $$X_n(t) = \hat{X}_n(t) + \tilde{X}_n(t)$$ where $$\hat{X}_n(t) = \sum_{k=1}^{\kappa} L_g(\frac{k}{n}) \sigma (t-\frac{k}{n}) \int_{t-\frac{k}{n}}^{t-\frac{k}{n}+\frac{1}{n}}(t-s)^\alpha dW(s)$$ and $$\tilde{X}_n(t) = \sum_{k=\kappa+1}^{N_n}g(\frac{b_k}{n})\sigma(t-\frac{k}{n})(W(t-\frac{k}{n}+\frac{1}{n})-W(t-\frac{k}{n})),$$ with $L_g$ chosen so that $g(t-s) \approx (t-s)^\alpha L_g(\frac{k}{n})$, \ \ $t-s \in [\frac{k-1}{n},\frac{k}{n}]$
\\
In terms of applying the scheme to rough volatility models, the Hybrid Scheme was used for option pricing in the Bergomi model via Monte Carlo simulation, as is discussed in the referenced paper. Using the Hybrid Scheme in this setting simplified the simulation process (\cite{see page 20 of BENNEDSON}) and reduced the computational cost compared to producing exact simulations from the model.  The results from using the scheme were then compared to exact results from the Bergomi model and it showed that the Hybrid Scheme was able to produce almost exactly the same volatility smile as that produced by exact simulation from the Bergomi model while being significantly more efficient.
\subsection{Markovian Representation of fBM}
\label{subsec:markovian_rep_fBm}
Without going into too much detail, (\cite{HARMS}) showed in (\cite{PAPER}) that fBM can be approximated by a Markovian representation consisting of the sum of $n$ weighted Orstein-Uhlenbeck processes.  Based on a set of assumptions outlined in the paper, the approximation is given by 
$$W_t^{H,n} = \sum_{i=1}^n w_{n,i} \int_0^t e^{-(t-s)x_{n,i}}dW_s, \ \ t \in [0,T]$$ 
where the $x_{n,i}$ are so-called 'speeds of mean reversion' and the $w_{n,i}$ are weights, with both terms taking positive values. This approximation forms part of a theorem which states that fBM is approximated by this representation at a rate of $n^{-r}$ for any given $r>0$.  This theorem also states that under this approximation, put prices in the Bergomi model also converge at a rate of $n^{-r}$. 
\\
\\
Using this approximation for fBM, a discrete Monte Carlo scheme for the rough Bergomi model can be acheived and since the representation is Markovian, it improves efficiency. It is also mentioned that in terms of error and complexity,  this method outperforms several other computational methods. However, it is outperformed by the Hybrid Scheme that was discussed earlier. 

\subsection{Extension of Donkser's Theorem to fBM}
Like the previous examples, the motivation of extending Donsker's theorem for Brownian motion to fBM is to be able to approximate fBM in a way that would reduce computational cost. To give some context,  we state Donsker's Theorem.
\\
\\
 \textbf{Theorem (Donsker)}: \textit{Let} $\epsilon_1,...,\epsilon_n$ \textit{be i.i.d. random variables with mean 0 and variance} $\sigma^2$. \textit{Then for} $X^{(n)}$ defined by
 
 \begin{equation}
 \label{eq:donkser_thm}
 X_t^{(n)} = \frac{1}{\sigma \sqrt{n}} (S_{\lfloor nt \rfloor} - (nt - \lfloor nt \rfloor) \epsilon_{\lfloor nt \rfloor + 1}), \ \ t\in[0,1]
 \end{equation}
 where,
\begin{equation}
S_m = \sum_{i=1}^i \epsilon_m
\end{equation}
then $X_t^{(n)} \rightarrow W_t$ \textit{in distribution for} $t\ge0$ \textit{on the space of continuous functions, where} $W_t$ \textit{is classical Brownian motion}.
\\

By being able to extend this theorem to fBM, one could approximate it using i.i.d. sequences of random variables leading to a simplified simulation process. Again, without going into too much detail, (\cite{HORVATH}) proved an extension of this theorem which approximates the logarithm of the stock price under a rough volatility model under the assumption that the i.i.d. random variables have a finite second moment (variance in this case) and a number of other conditions which we will omit but can be found in the paper. In terms of pricing, this approximation could then be applied to fractional binomial trees, which opens the door to pricing American-type options under rough volatility models. However, the branches of such trees generally do not recombine which adds to the complexity of this pricing method.   
use of several pricing tools such as the Black-Scholes formula and other pricing PDE's due to the non-deterministic nature of the volatility process.  Pricing via Monte Carlo simulation of the asset price and volatility processes is perhaps the one pricing tool that works but it can be slow in approximating continuous time solutions within a sufficient level of accuracy, especially for simulating non-Markovian process due to the extra memory required to store previous process states. Here, we give a brief overview of some theories that were introduced to combat the computational expense of exact Monte Carlo simulation of the volatility process. Namely, we discuss the Hybrid scheme from (\cite{BENNEDSON}), the Markovian representation of fBM in (\cite{HARMS}) and the extension of the Donsker theorem to fBM in (\cite{MUGURUZA}). These methods were introduced to improve the efficiency of the simulation of fBM and volatility processes while maintaining a reasonable level of accuracy.

\subsection{The Hybrid Scheme}
The Hybrid Scheme was introduced by (\cite{BENNEDSON}) in (\cite{PAPER}) in which they study simulation methods for Brownian semistationary processes. A brownian semistationary process can be written as $$X(t)=\int_{-\infty}^t g(t-s) \sigma(s) dW(s),  \ \  t\in\mathbb{R}$$ where $\sigma$ is a predictable process with respect to the given filatration. It is mentioned in the paper that when $g(x) \propto x^\alpha$ and $\alpha \in (-\frac{1}{2}, \frac{1}{2}) \setminus \{0\}$, the process behaves (locally) like a fBM with Hurst paramter $H=\alpha+1/2$.  Therefore, being able to efficiently simulate such a process would help to improve computational efficiency for pricing in some rough volatility models.  
\\
\\
In the referenced paper, a method for simulating such a process was introduced. The given scheme involves approximating the function $g$ using a step function except for near 0, where a power function is used for approximation. Through a series of derivations,  the resulting discretisation scheme is written as a linear combination of a Riemann sum and Wiener integrals. The scheme is given by: $$X_n(t) = \hat{X}_n(t) + \tilde{X}_n(t)$$ where $$\hat{X}_n(t) = \sum_{k=1}^{\kappa} L_g(\frac{k}{n}) \sigma (t-\frac{k}{n}) \int_{t-\frac{k}{n}}^{t-\frac{k}{n}+\frac{1}{n}}(t-s)^\alpha dW(s)$$ and $$\tilde{X}_n(t) = \sum_{k=\kappa+1}^{N_n}g(\frac{b_k}{n})\sigma(t-\frac{k}{n})(W(t-\frac{k}{n}+\frac{1}{n})-W(t-\frac{k}{n})),$$ with $L_g$ chosen so that $g(t-s) \approx (t-s)^\alpha L_g(\frac{k}{n})$, \ \ $t-s \in [\frac{k-1}{n},\frac{k}{n}]$
\\
In terms of applying the scheme to rough volatility models, the Hybrid Scheme was used for option pricing in the Bergomi model via Monte Carlo simulation, as is discussed in the referenced paper. Using the Hybrid Scheme in this setting simplified the simulation process (\cite{see page 20 of BENNEDSON}) and reduced the computational cost compared to producing exact simulations from the model.  The results from using the scheme were then compared to exact results from the Bergomi model and it showed that the Hybrid Scheme was able to produce almost exactly the same volatility smile as that produced by exact simulation from the Bergomi model while being significantly more efficient.
\subsection{Markovian Representation of fBM}
Without going into too much detail, (\cite{HARMS}) showed in (\cite{PAPER}) that fBM can be approximated by a Markovian representation consisting of the sum of $n$ weighted Orstein-Uhlenbeck processes.  Based on a set of assumptions outlined in the paper, the approximation is given by $$W_t^{H,n} = \sum_{i=1}^n w_{n,i} \int_0^t e^{-(t-s)x_{n,i}}dW_s, \ \ t \in [0,T]$$ where the $x_{n,i}$ are so-called 'speeds of mean reversion' and the $w_{n,i}$ are weights, with both terms taking positive values. This approximation forms part of a theorem which states that fBM is approximated by this representation at a rate of $n^{-r}$ for any given $r>0$.  This theorem also states that under this approximation, put prices in the Bergomi model also converge at a rate of $n^{-r}$. 
\\
\\
Using this approximation for fBM, a discrete Monte Carlo scheme for the rough Bergomi model can be acheived and since the representation is Markovian, it improves efficiency. It is also mentioned that in terms of error and complexity,  this method outperforms several other computational methods. However, it is outperformed by the Hybrid Scheme that was discussed earlier. 

\subsection{Extension of Donkser's Theorem to fBM}
Like the previous examples, the motivation of extending Donsker's theorem for Brownian motion to fBM is to be able to approximate fBM in a way that would reduce computational cost. To give some context,  we state Donsker's Theorem.
\\
\\
 \textbf{Theorem (Donsker)}: \textit{Let} $\epsilon_1,...,\epsilon_n$ \textit{be i.i.d. random variables with mean 0 and variance} $\sigma^2$. \textit{Then for} $X^{(n)}$ \textit{defined by} $$X_t^{(n)} = \frac{1}{\sigma \sqrt{n}} (S_{\lfloor nt \rfloor} - (nt - \lfloor nt \rfloor) \epsilon_{\lfloor nt \rfloor + 1}), \ \ t\in[0,1],$$ \textit{where} $$S_m = \sum_{i=1}^i \epsilon_m,$$ \textit{then} $X_t^{(n)} \rightarrow W_t$ \textit{in distribution for} $t\ge0$ \textit{on the space of continuous functions, where} $W_t$ \textit{is classical Brownian motion}.
\\
\\
By being able to extend this theorem to fBM, one could approximate it using i.i.d. sequences of random variables leading to a simplified simulation process. Again, without going into too much detail, (\cite{HORVATH}) proved an extension of this theorem which approximates the logarithm of the stock price under a rough volatility model under the assumption that the i.i.d. random variables have a finite second moment (variance in this case) and a number of other conditions which we will omit but can be found in the paper. In terms of pricing, this approximation could then be applied to fractional binomial trees, which opens the door to pricing American-type options under rough volatility models. However, the branches of such trees generally do not recombine which adds to the complexity of this pricing method.   

\section{Conclusion}
\label{sec:conclusion}
In the vast majority of papers this section is not present. It could be used to provide a brief summary of the paper or some areas for further research. Most likely it won't be useful for the final essays.

The final point in this document is of high importance. 
We should remember that whatever we write, the first version of the text is always far from perfect. 
It is critical to work on the text after the first version is created and rewrite it where it could be improved. 
 \bibliographystyle{plain}  % Or use the `amsrefs' package (http://www.ams.org/tex/amsrefs.html)!
\bibliography{my_bibtex_file.bib}
 \addcontentsline{toc}{section}{Bibliography}
\end{document}

 ---------------